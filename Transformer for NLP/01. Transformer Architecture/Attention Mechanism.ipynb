{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MultiHead Attention Sublayer**\n",
    "\n",
    "- The input of the multi-attention sublayer of the first layer of the encoder stack is a vector that \n",
    "contains the embedding and the positional encoding of each word. The next layers of the stack \n",
    "do not start these operations over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ LOADING THE REQUIRED LIBRARIES AND DEPENDENCIES\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Represent the Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input: 3 inputs, d_model=4\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#@ REPRESENT THE INPUT\n",
    "print(\"Step 1: Input: 3 inputs, d_model=4\")\n",
    "\n",
    "x = np.array([[1.0, 0.0, 1.0, 0.0],              # Input 1\n",
    "              [0.0, 2.0, 0.0, 2.0,],             # Input 2\n",
    "              [1.0, 1.0, 1.0, 1.0]])             # Input 3\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Initializing the weight matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Weight 3 Dimensions x d_model=4\n",
      "W_query\n",
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#@ INITIALIZING THE WEIGHT MATRICES\n",
    "print(\"Step 2: Weight 3 Dimensions x d_model=4\")\n",
    "print(\"W_query\")\n",
    "w_query = np.array([[1, 0, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 1]])\n",
    "\n",
    "print(w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_key\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W_key\")\n",
    "w_key =np.array([[0, 0, 1],\n",
    "                 [1, 1, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 1, 0]])\n",
    "print(w_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_value\n",
      "[[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "                    [0, 3, 0],\n",
    "                    [1, 0, 3],\n",
    "                    [1, 1, 0]])\n",
    "\n",
    "print(w_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Matrix Multiplication to obtain Q, K, and V**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: x * w_query\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Queries: x * w_query\")\n",
    "Q = np.matmul(x, w_query)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: x * w_key\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys: x * w_key\")\n",
    "K = np.matmul(x, w_key)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: x * w_value\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Values: x * w_value\")\n",
    "V = np.matmul(x, w_value)\n",
    "print(V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Scaled Attention Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "#@ SCALED ATTENTION SCORES\n",
    "k_d = 1                      # Square root of k_d\n",
    "attention_scores = (Q @ K.transpose()/k_d)\n",
    "print(attention_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Scaled Softmax Attention Scores for Each Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "#@ SCALED SOFTMAX ATTENTION SCORES FOR EACH VECTOR\n",
    "attention_scores[0]=softmax(attention_scores[0])\n",
    "attention_scores[1]=softmax(attention_scores[1])\n",
    "attention_scores[2]=softmax(attention_scores[2])\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: The Final Attention Representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention 1\n",
      "[0.06337894 0.12675788 0.19013681]\n",
      "\n",
      "Attention 2\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "\n",
      "Attention 3\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "#@ ATTENTION REPRESENTATIONS\n",
    "print(\"Attention 1\")\n",
    "attention1=attention_scores[0].reshape(-1,1)\n",
    "attention1=attention_scores[0][0]*V[0]\n",
    "print(attention1)\n",
    "\n",
    "print(\"\\nAttention 2\")\n",
    "attention2=attention_scores[0][1]*V[1]\n",
    "print(attention2)\n",
    "\n",
    "print(\"\\nAttention 3\")\n",
    "attention3=attention_scores[0][2]*V[2]\n",
    "print(attention3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Summing up the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "#@ SUM THE RESULTS TO CREATE OUTPUT MATRIX\n",
    "attention_input1 = attention1+attention2+attention3\n",
    "print(attention_input1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step8: Do the Similar steps for inputs 1 to 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.51803227 0.06593474 0.57898974 0.26156816 0.45903392 0.54301963\n",
      "  0.36944504 0.78062509 0.84132218 0.80327461 0.76400561 0.53534402\n",
      "  0.68802208 0.3250726  0.94307388 0.56236725 0.71137752 0.70393443\n",
      "  0.80570251 0.07422293 0.80974836 0.93014795 0.14419805 0.91441808\n",
      "  0.64473565 0.7361935  0.97487457 0.83497554 0.99635858 0.55097136\n",
      "  0.18568183 0.68184303 0.20092957 0.64811001 0.53270291 0.63644098\n",
      "  0.49144787 0.75809754 0.50862987 0.91943953 0.77325465 0.33735139\n",
      "  0.76314969 0.93319312 0.78110282 0.1827154  0.53686553 0.45725193\n",
      "  0.35386854 0.6857252  0.37433003 0.69211331 0.47597565 0.2182687\n",
      "  0.52112479 0.46600617 0.0706057  0.76639653 0.2188229  0.05410546\n",
      "  0.77817158 0.06417579 0.11750989 0.12067076]\n",
      " [0.78121092 0.55654516 0.11760405 0.80981876 0.97131738 0.74502757\n",
      "  0.9683498  0.35195239 0.13098691 0.77428144 0.99673375 0.9935002\n",
      "  0.2529348  0.02599951 0.50620214 0.43188773 0.05327587 0.22232563\n",
      "  0.19142361 0.20829672 0.15946058 0.1350743  0.91098839 0.60906112\n",
      "  0.80697821 0.94637329 0.77402282 0.95100686 0.27651009 0.00451453\n",
      "  0.3259895  0.73460888 0.75730524 0.2428098  0.48799325 0.25493374\n",
      "  0.36564954 0.98873801 0.15188389 0.96652151 0.524815   0.58198067\n",
      "  0.13794491 0.49258128 0.07787729 0.3235866  0.16415719 0.06926274\n",
      "  0.89786647 0.46543146 0.38368262 0.35232329 0.66806267 0.38431827\n",
      "  0.06478945 0.72244265 0.88444599 0.28467087 0.3077418  0.82592436\n",
      "  0.88917818 0.37302521 0.06415834 0.81161507]\n",
      " [0.5268594  0.91383264 0.08423467 0.29548533 0.30526149 0.77862819\n",
      "  0.00530319 0.86407617 0.87639887 0.22573989 0.56747805 0.14685386\n",
      "  0.83014716 0.3664583  0.60126989 0.65532226 0.71866581 0.3370956\n",
      "  0.08501867 0.3787679  0.42539234 0.38558731 0.90410246 0.96766921\n",
      "  0.71579476 0.23478187 0.88194207 0.98588814 0.46676941 0.52930877\n",
      "  0.1892447  0.20788342 0.91333353 0.90007833 0.4747735  0.34684393\n",
      "  0.78307502 0.07412515 0.1244462  0.85189595 0.27445653 0.05757973\n",
      "  0.98046195 0.91010217 0.46854205 0.01819244 0.66452598 0.18920912\n",
      "  0.07877923 0.35486414 0.36179499 0.25207539 0.71986995 0.8573091\n",
      "  0.28991801 0.05415336 0.63916162 0.93369038 0.24190104 0.9122488\n",
      "  0.79240607 0.34180135 0.73047662 0.35855676]]\n"
     ]
    }
   ],
   "source": [
    "attention_head1=np.random.random((3, 64))\n",
    "print(attention_head1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9: The Output of Heads of Attention Sublayer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one head:  (3, 64)\n",
      "Dimension of 8 heads: 512\n"
     ]
    }
   ],
   "source": [
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "print(\"Shape of one head: \",z0h1.shape)\n",
    "print(\"Dimension of 8 heads:\" ,64*8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 10: Concatenation of Output of the Heads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.63543689 0.26536128 0.82749327 ... 0.83781243 0.17844576 0.72299349]\n",
      " [0.66922363 0.25694862 0.71219946 ... 0.93571269 0.82527417 0.44343282]\n",
      " [0.16468276 0.13075808 0.15698728 ... 0.3454452  0.03596274 0.93268568]]\n"
     ]
    }
   ],
   "source": [
    "#@ CONCATENATING THE HEADS OF THE OUTPUTS\n",
    "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Bonjour, je suis Saugat Regmi, en cours d'apprentissage Machine Learning\"}]\n"
     ]
    }
   ],
   "source": [
    "#@ IMPLEMENTING THE TRANSFORMER MODEL FROM HUGGING FACE\n",
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"Hello, I am Saugat Regmi, Currently learning Machine Learning\", max_length=50))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merci**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformersNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
