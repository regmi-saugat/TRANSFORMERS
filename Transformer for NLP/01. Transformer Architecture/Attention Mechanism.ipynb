{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MultiHead Attention Sublayer**\n",
    "\n",
    "- The input of the multi-attention sublayer of the first layer of the encoder stack is a vector that \n",
    "contains the embedding and the positional encoding of each word. The next layers of the stack \n",
    "do not start these operations over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@ LOADING THE REQUIRED LIBRARIES AND DEPENDENCIES\n",
    "import numpy as np\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Represent the Input**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Input: 3 inputs, d_model=4\n",
      "[[1. 0. 1. 0.]\n",
      " [0. 2. 0. 2.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#@ REPRESENT THE INPUT\n",
    "print(\"Step 1: Input: 3 inputs, d_model=4\")\n",
    "\n",
    "x = np.array([[1.0, 0.0, 1.0, 0.0],              # Input 1\n",
    "              [0.0, 2.0, 0.0, 2.0,],             # Input 2\n",
    "              [1.0, 1.0, 1.0, 1.0]])             # Input 3\n",
    "\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Initializing the weight matrices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Weight 3 Dimensions x d_model=4\n",
      "W_query\n",
      "[[1 0 1]\n",
      " [1 0 0]\n",
      " [0 0 1]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#@ INITIALIZING THE WEIGHT MATRICES\n",
    "print(\"Step 2: Weight 3 Dimensions x d_model=4\")\n",
    "print(\"W_query\")\n",
    "w_query = np.array([[1, 0, 1],\n",
    "                   [1, 0, 0],\n",
    "                   [0, 0, 1],\n",
    "                   [0, 1, 1]])\n",
    "\n",
    "print(w_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_key\n",
      "[[0 0 1]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W_key\")\n",
    "w_key =np.array([[0, 0, 1],\n",
    "                 [1, 1, 0],\n",
    "                 [0, 1, 0],\n",
    "                 [1, 1, 0]])\n",
    "print(w_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_value\n",
      "[[0 2 0]\n",
      " [0 3 0]\n",
      " [1 0 3]\n",
      " [1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"W_value\")\n",
    "w_value = np.array([[0, 2, 0],\n",
    "                    [0, 3, 0],\n",
    "                    [1, 0, 3],\n",
    "                    [1, 1, 0]])\n",
    "\n",
    "print(w_value)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Matrix Multiplication to obtain Q, K, and V**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: x * w_query\n",
      "[[1. 0. 2.]\n",
      " [2. 2. 2.]\n",
      " [2. 1. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Queries: x * w_query\")\n",
    "Q = np.matmul(x, w_query)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: x * w_key\n",
      "[[0. 1. 1.]\n",
      " [4. 4. 0.]\n",
      " [2. 3. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Keys: x * w_key\")\n",
    "K = np.matmul(x, w_key)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values: x * w_value\n",
      "[[1. 2. 3.]\n",
      " [2. 8. 0.]\n",
      " [2. 6. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Values: x * w_value\")\n",
    "V = np.matmul(x, w_value)\n",
    "print(V)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Scaled Attention Scores**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.  4.  4.]\n",
      " [ 4. 16. 12.]\n",
      " [ 4. 12. 10.]]\n"
     ]
    }
   ],
   "source": [
    "#@ SCALED ATTENTION SCORES\n",
    "k_d = 1                      # Square root of k_d\n",
    "attention_scores = (Q @ K.transpose()/k_d)\n",
    "print(attention_scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Scaled Softmax Attention Scores for Each Vector**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06337894 0.46831053 0.46831053]\n",
      "[6.03366485e-06 9.82007865e-01 1.79861014e-02]\n",
      "[2.95387223e-04 8.80536902e-01 1.19167711e-01]\n"
     ]
    }
   ],
   "source": [
    "#@ SCALED SOFTMAX ATTENTION SCORES FOR EACH VECTOR\n",
    "attention_scores[0]=softmax(attention_scores[0])\n",
    "attention_scores[1]=softmax(attention_scores[1])\n",
    "attention_scores[2]=softmax(attention_scores[2])\n",
    "print(attention_scores[0])\n",
    "print(attention_scores[1])\n",
    "print(attention_scores[2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: The Final Attention Representation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention 1\n",
      "[0.06337894 0.12675788 0.19013681]\n",
      "\n",
      "Attention 2\n",
      "[0.93662106 3.74648425 0.        ]\n",
      "\n",
      "Attention 3\n",
      "[0.93662106 2.80986319 1.40493159]\n"
     ]
    }
   ],
   "source": [
    "#@ ATTENTION REPRESENTATIONS\n",
    "print(\"Attention 1\")\n",
    "attention1=attention_scores[0].reshape(-1,1)\n",
    "attention1=attention_scores[0][0]*V[0]\n",
    "print(attention1)\n",
    "\n",
    "print(\"\\nAttention 2\")\n",
    "attention2=attention_scores[0][1]*V[1]\n",
    "print(attention2)\n",
    "\n",
    "print(\"\\nAttention 3\")\n",
    "attention3=attention_scores[0][2]*V[2]\n",
    "print(attention3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 7: Summing up the Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.93662106 6.68310531 1.59506841]\n"
     ]
    }
   ],
   "source": [
    "#@ SUM THE RESULTS TO CREATE OUTPUT MATRIX\n",
    "attention_input1 = attention1+attention2+attention3\n",
    "print(attention_input1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step8: Do the Similar steps for inputs 1 to 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.02968061e-01 4.78933152e-01 1.55845508e-01 9.88262330e-01\n",
      "  9.76938629e-01 9.78654027e-01 3.77457489e-01 5.06229302e-01\n",
      "  7.51550229e-01 9.59441860e-01 5.70817809e-02 1.31432354e-01\n",
      "  4.02944707e-01 4.06536263e-01 8.06707901e-01 4.55212756e-01\n",
      "  3.80748823e-01 5.77114545e-01 9.74070787e-01 7.08102374e-01\n",
      "  8.83868278e-01 9.02558661e-01 2.77232430e-02 7.30864194e-01\n",
      "  6.57596439e-01 7.69553162e-01 7.95469750e-01 6.14468045e-01\n",
      "  3.20692290e-01 1.42336004e-01 2.06866035e-02 4.26051180e-02\n",
      "  3.18676726e-01 1.55514270e-01 9.05085397e-01 1.74706790e-02\n",
      "  4.87706995e-02 9.58028477e-01 7.92824838e-01 3.00061095e-01\n",
      "  5.08009776e-02 1.49978213e-01 5.51797925e-01 1.13682038e-01\n",
      "  7.75111448e-02 1.59331427e-01 9.06658179e-01 2.81491899e-01\n",
      "  6.09717204e-01 7.63891560e-01 2.43232169e-01 5.17703150e-01\n",
      "  6.81168058e-01 5.52535938e-01 3.96887037e-01 2.93843453e-02\n",
      "  4.57757113e-01 9.16622655e-01 2.66865969e-02 2.60963474e-01\n",
      "  3.88968526e-01 7.36779249e-01 9.05427539e-02 9.92435488e-01]\n",
      " [8.93331157e-01 8.77782478e-01 8.42944582e-01 1.58757133e-01\n",
      "  9.43807514e-01 2.62484171e-01 5.59709889e-02 7.33658715e-01\n",
      "  8.18892035e-01 8.25179534e-01 8.25327635e-02 4.33101288e-01\n",
      "  2.32607035e-01 4.63930430e-01 9.02604660e-01 8.58294087e-01\n",
      "  8.22900984e-01 1.03429110e-01 4.79302609e-01 2.35656512e-01\n",
      "  6.53601559e-01 1.92580347e-01 6.21083920e-01 6.08190059e-01\n",
      "  7.50036522e-01 4.66844398e-01 9.02972224e-02 9.27718434e-02\n",
      "  9.19469365e-01 5.94736257e-01 9.74940165e-01 6.48399662e-01\n",
      "  5.63611171e-01 4.68118227e-01 7.94370614e-01 6.53143922e-01\n",
      "  9.23458232e-01 8.13690522e-01 5.46030328e-01 4.93214651e-01\n",
      "  1.85700912e-01 9.01860912e-01 4.64553626e-01 4.77813199e-03\n",
      "  8.10386179e-01 3.27429947e-01 5.91807154e-01 8.34127347e-01\n",
      "  5.25848078e-01 4.70984208e-02 5.61471455e-01 4.96313432e-01\n",
      "  6.45267283e-01 6.28013820e-01 1.31596823e-01 2.44989855e-01\n",
      "  8.89549142e-01 4.42136784e-01 1.29379162e-01 6.37808964e-03\n",
      "  1.78953482e-01 1.14356711e-01 9.85222224e-01 6.88265518e-04]\n",
      " [4.82358263e-01 4.47839064e-01 8.63506470e-01 9.78511727e-02\n",
      "  5.42309293e-01 3.41838502e-01 1.68690717e-01 8.14544381e-01\n",
      "  8.75141328e-01 8.86089880e-01 3.56799600e-01 6.25669561e-01\n",
      "  1.29982101e-01 1.08823300e-01 7.84418210e-01 6.85516424e-01\n",
      "  8.48280936e-01 9.14291659e-01 1.69717117e-01 8.34550800e-01\n",
      "  2.17708035e-01 8.37723070e-01 1.99408689e-01 9.96734011e-01\n",
      "  9.24320846e-01 3.88903115e-02 4.45795987e-01 2.79257882e-01\n",
      "  6.66801976e-01 9.55929330e-01 7.91686737e-01 1.11775769e-01\n",
      "  9.16944772e-01 8.63012059e-01 1.04803164e-01 3.98286179e-01\n",
      "  5.15294120e-01 1.00833493e-01 8.86122870e-01 5.91871806e-01\n",
      "  1.00524882e-01 2.09641165e-01 2.67074324e-01 3.77912275e-01\n",
      "  8.93017068e-01 8.27349369e-01 4.44518282e-01 3.15138263e-02\n",
      "  1.54799162e-01 5.27504429e-02 8.95769318e-01 1.19119755e-01\n",
      "  7.48583716e-01 2.93242266e-01 2.60129463e-01 4.10558248e-01\n",
      "  8.31117331e-01 5.37373568e-01 6.87541696e-01 7.16436526e-01\n",
      "  7.88208781e-01 3.43564084e-01 3.49693391e-01 6.44391520e-01]]\n"
     ]
    }
   ],
   "source": [
    "attention_head1=np.random.random((3, 64))\n",
    "print(attention_head1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 9: The Output of Heads of Attention Sublayer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of one head:  (3, 64)\n",
      "Dimension of 8 heads: 512\n"
     ]
    }
   ],
   "source": [
    "z0h1=np.random.random((3, 64))\n",
    "z1h2=np.random.random((3, 64))\n",
    "z2h3=np.random.random((3, 64))\n",
    "z3h4=np.random.random((3, 64))\n",
    "z4h5=np.random.random((3, 64))\n",
    "z5h6=np.random.random((3, 64))\n",
    "z6h7=np.random.random((3, 64))\n",
    "z7h8=np.random.random((3, 64))\n",
    "print(\"Shape of one head: \",z0h1.shape)\n",
    "print(\"Dimension of 8 heads:\" ,64*8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 10: Concatenation of Output of the Heads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.98440752 0.14025898 0.55047621 ... 0.3296116  0.31675955 0.78442565]\n",
      " [0.73133939 0.64405049 0.46110023 ... 0.56983026 0.56319405 0.46128129]\n",
      " [0.44253367 0.41572166 0.62073751 ... 0.76825721 0.88000215 0.23723273]]\n"
     ]
    }
   ],
   "source": [
    "#@ CONCATENATING THE HEADS OF THE OUTPUTS\n",
    "output_attention=np.hstack((z0h1,z1h2,z2h3,z3h4,z4h5,z5h6,z6h7,z7h8))\n",
    "print(output_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': 'Bonjour, je suis Saugat Regmi.'}]\n"
     ]
    }
   ],
   "source": [
    "#@ IMPLEMENTING THE TRANSFORMER MODEL FROM HUGGING FACE\n",
    "from transformers import pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\")\n",
    "print(translator(\"Hello, I am Saugat Regmi. Currently learning Machine Learning\", max_length=50))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**merci**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformersNLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
