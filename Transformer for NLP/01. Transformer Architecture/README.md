The flexible, orginal Transformer architecture provides the basis for many other innovative variations that open the way for more powerful transduction problems and language modelings.

[This notebook](https://github.com/regmi-saugat/TRANSFORMERS/blob/main/Transformer%20for%20NLP/01.%20Transformer%20Architecture/Attention%20Mechanism.ipynb) contains the implementation of the **Architecture of Multi-head Attention Mechanisms**
